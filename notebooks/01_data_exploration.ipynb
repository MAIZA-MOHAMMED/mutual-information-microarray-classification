# %% [markdown]
# # Microarray Data Exploration Notebook
# ## Loading and Understanding the Datasets from the Paper

# %%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# %%
# Dataset information from Table 1 in the paper
datasets_info = {
    'Leukemia': {
        'genes': 7129,
        'training': 38,
        'testing': 34,
        'observations': '27/11 – 20/14'
    },
    'Brain_Cancer': {
        'genes': 10367,
        'training': 60,
        'testing': 30,
        'observations': '50/40'
    },
    'Colon_Cancer': {
        'genes': 2000,
        'training': 42,
        'testing': 20,
        'observations': '22/40'
    },
    'SRBCT': {
        'genes': 2308,
        'training': 63,
        'testing': 20,
        'observations': '43/40'
    },
    'Prostate_Tumor': {
        'genes': 12600,
        'training': 102,
        'testing': None,
        'observations': '52/50'
    },
    'Lung_Cancer': {
        'genes': 12533,
        'training': 144,
        'testing': 59,
        'observations': '139/64'
    },
    'Lymphoma': {
        'genes': 4026,
        'training': 60,
        'testing': 36,
        'observations': '45/15 – 27/9'
    },
    '11_Tumors': {
        'genes': 4200,
        'training': 119,
        'testing': 55,
        'observations': '94/80'
    },
    'DLBCL': {
        'genes': 3812,
        'training': 42,
        'testing': None,
        'observations': '3/39'
    }
}

# %%
# Create summary dataframe
summary_data = []
for name, info in datasets_info.items():
    summary_data.append({
        'Dataset': name,
        'Genes': info['genes'],
        'Training Samples': info['training'],
        'Testing Samples': info['testing'] if info['testing'] else 'N/A',
        'Total Samples': info['training'] + (info['testing'] if info['testing'] else 0),
        'Observations': info['observations']
    })

summary_df = pd.DataFrame(summary_data)
print("Dataset Summary from Paper:")
print(summary_df.to_string())

# %%
# Visualization: Dataset characteristics
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# Plot 1: Genes vs Samples
axes[0,0].scatter(summary_df['Total Samples'], summary_df['Genes'], s=100, alpha=0.7)
for i, row in summary_df.iterrows():
    axes[0,0].annotate(row['Dataset'], 
                      (row['Total Samples'], row['Genes']),
                      xytext=(5, 5), textcoords='offset points',
                      fontsize=9)
axes[0,0].set_xlabel('Total Samples')
axes[0,0].set_ylabel('Number of Genes')
axes[0,0].set_title('Genes vs Samples (High-Dimensional Data)')
axes[0,0].set_yscale('log')
axes[0,0].grid(True, alpha=0.3)

# Plot 2: Training vs Testing distribution
datasets_with_test = summary_df[summary_df['Testing Samples'] != 'N/A']
train_test_ratio = []
for idx, row in datasets_with_test.iterrows():
    train = row['Training Samples']
    test = row['Testing Samples']
    ratio = train / (train + test)
    train_test_ratio.append(ratio)

axes[0,1].bar(range(len(datasets_with_test)), train_test_ratio, color='steelblue')
axes[0,1].set_xticks(range(len(datasets_with_test)))
axes[0,1].set_xticklabels(datasets_with_test['Dataset'], rotation=45, ha='right')
axes[0,1].set_ylabel('Training Ratio')
axes[0,1].set_title('Training/Testing Split Ratio')
axes[0,1].axhline(y=0.8, color='red', linestyle='--', alpha=0.5, label='80% train')
axes[0,1].legend()
axes[0,1].grid(True, alpha=0.3, axis='y')

# Plot 3: Dimensionality (p/n ratio)
summary_df['p_n_ratio'] = summary_df['Genes'] / summary_df['Total Samples']
axes[0,2].barh(summary_df['Dataset'], summary_df['p_n_ratio'], color='forestgreen')
axes[0,2].set_xlabel('Features/Samples Ratio (p/n)')
axes[0,2].set_title('High-Dimensionality Challenge')
axes[0,2].axvline(x=100, color='red', linestyle='--', alpha=0.5, label='p/n = 100')
axes[0,2].legend()
axes[0,2].grid(True, alpha=0.3)

# Plot 4: Class distribution (simplified from observations)
class_imbalance = []
for obs in summary_df['Observations']:
    # Parse observations like "27/11 – 20/14" or "50/40"
    if '–' in obs:
        parts = obs.split('–')
        first_part = parts[0].strip()
        if '/' in first_part:
            pos, neg = map(int, first_part.split('/'))
            imbalance_ratio = min(pos, neg) / max(pos, neg)
            class_imbalance.append(imbalance_ratio)
    elif '/' in obs:
        try:
            pos, neg = map(int, obs.split('/'))
            imbalance_ratio = min(pos, neg) / max(pos, neg)
            class_imbalance.append(imbalance_ratio)
        except:
            class_imbalance.append(1.0)
    else:
        class_imbalance.append(1.0)

summary_df['Class_Imbalance'] = class_imbalance
axes[1,0].bar(summary_df['Dataset'], summary_df['Class_Imbalance'], color='coral')
axes[1,0].set_xticklabels(summary_df['Dataset'], rotation=45, ha='right')
axes[1,0].set_ylabel('Class Balance Ratio')
axes[1,0].set_title('Class Imbalance Analysis')
axes[1,0].set_ylim(0, 1)
axes[1,0].grid(True, alpha=0.3, axis='y')

# Plot 5: Dataset sizes
axes[1,1].pie(summary_df['Genes'], labels=summary_df['Dataset'], autopct='%1.1f%%',
             startangle=90, radius=1.2)
axes[1,1].set_title('Distribution of Feature Dimensions')

# Plot 6: Complexity heatmap
complexity_matrix = summary_df[['Genes', 'Total Samples', 'p_n_ratio', 'Class_Imbalance']].copy()
complexity_matrix.index = summary_df['Dataset']
# Normalize for heatmap
complexity_normalized = (complexity_matrix - complexity_matrix.min()) / (complexity_matrix.max() - complexity_matrix.min())

sns.heatmap(complexity_normalized.T, annot=True, fmt='.2f', cmap='YlOrRd',
           cbar_kws={'label': 'Normalized Value'}, ax=axes[1,2])
axes[1,2].set_title('Dataset Complexity Heatmap')
axes[1,2].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.savefig('../results/dataset_analysis_paper.png', dpi=300, bbox_inches='tight')
plt.show()

# %%
# Load and analyze actual data (if available)
def load_simulated_data(dataset_name, info):
    """Generate simulated data for demonstration"""
    n_samples = info['training'] + (info['testing'] if info['testing'] else 0)
    n_features = info['genes']
    
    # Simulate gene expression data with realistic patterns
    np.random.seed(42)
    
    # Create informative features (10% of total)
    n_informative = int(n_features * 0.1)
    
    # Generate two classes with different means for informative features
    X = np.random.randn(n_samples, n_features)
    y = np.random.randint(0, 2, n_samples)
    
    # Add signal to informative features
    for i in range(n_informative):
        X[y == 0, i] += 1.5  # Class 0 has higher expression
        X[y == 1, i] -= 1.5  # Class 1 has lower expression
    
    # Add some correlated features
    for i in range(n_informative, n_informative + 100):
        X[:, i] = 0.7 * X[:, i-1] + 0.3 * np.random.randn(n_samples)
    
    # Add noise to remaining features
    X[:, n_informative+100:] += np.random.randn(n_samples, n_features - (n_informative+100)) * 0.5
    
    return X, y

# %%
# Generate and analyze simulated Leukemia data
print("\n" + "="*60)
print("Analyzing Simulated Leukemia Dataset")
print("="*60)

X_leuk, y_leuk = load_simulated_data('Leukemia', datasets_info['Leukemia'])
print(f"Generated data shape: {X_leuk.shape}")
print(f"Class distribution: Class 0: {sum(y_leuk==0)}, Class 1: {sum(y_leuk==1)}")

# %%
# Basic statistics
print("\nBasic Statistics:")
print(f"Mean: {X_leuk.mean():.4f}")
print(f"Std: {X_leuk.std():.4f}")
print(f"Min: {X_leuk.min():.4f}")
print(f"Max: {X_leuk.max():.4f}")
print(f"Missing values: {np.isnan(X_leuk).sum()}")

# %%
# Feature distribution analysis
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.hist(X_leuk.flatten(), bins=100, alpha=0.7, color='blue', edgecolor='black')
plt.xlabel('Gene Expression Value')
plt.ylabel('Frequency')
plt.title('Overall Distribution of Gene Expressions')
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 2)
# Distribution by class
plt.hist(X_leuk[y_leuk == 0].flatten(), bins=50, alpha=0.5, label='Class 0', color='blue')
plt.hist(X_leuk[y_leuk == 1].flatten(), bins=50, alpha=0.5, label='Class 1', color='red')
plt.xlabel('Gene Expression Value')
plt.ylabel('Frequency')
plt.title('Distribution by Class')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 3)
# Top 10 most variable genes
gene_variance = np.var(X_leuk, axis=0)
top_var_indices = np.argsort(gene_variance)[-10:]
for idx in top_var_indices:
    plt.hist(X_leuk[:, idx], bins=30, alpha=0.3, label=f'Gene {idx}')
plt.xlabel('Expression Value')
plt.ylabel('Frequency')
plt.title('Top 10 Most Variable Genes')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('../results/leukemia_distributions.png', dpi=300)
plt.show()

# %%
# Correlation analysis
print("\nAnalyzing feature correlations...")

# Calculate correlation matrix for a subset of features
np.random.seed(42)
sample_indices = np.random.choice(X_leuk.shape[1], 50, replace=False)
corr_matrix = np.corrcoef(X_leuk[:, sample_indices].T)

plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, cmap='coolwarm', center=0,
           xticklabels=False, yticklabels=False,
           cbar_kws={'label': 'Correlation'})
plt.title('Correlation Matrix of 50 Random Genes (Leukemia)')
plt.tight_layout()
plt.savefig('../results/correlation_analysis.png', dpi=300)
plt.show()

# %%
# PCA visualization
print("\nPerforming PCA analysis...")

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_leuk)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(10, 8))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_leuk, 
                     cmap='viridis', s=100, alpha=0.7, edgecolor='black')
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')
plt.title('PCA: Leukemia Dataset (Simulated)')
plt.colorbar(scatter, label='Class')
plt.grid(True, alpha=0.3)

# Add convex hull for each class
from scipy.spatial import ConvexHull
for class_label in [0, 1]:
    points = X_pca[y_leuk == class_label]
    if len(points) > 2:  # Need at least 3 points for convex hull
        hull = ConvexHull(points)
        for simplex in hull.simplices:
            plt.plot(points[simplex, 0], points[simplex, 1], 
                    color='red' if class_label == 0 else 'blue',
                    linewidth=2, alpha=0.3)

plt.savefig('../results/pca_leukemia.png', dpi=300)
plt.show()

# %%
# Variance explained by PCA
pca_full = PCA()
pca_full.fit(X_scaled)
explained_variance = np.cumsum(pca_full.explained_variance_ratio_)

plt.figure(figsize=(10, 6))
plt.plot(range(1, len(explained_variance) + 1), explained_variance,
         marker='o', linewidth=2, markersize=5)
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('PCA: Variance Explained')
plt.grid(True, alpha=0.3)
plt.axhline(y=0.95, color='red', linestyle='--', label='95% variance')
plt.axhline(y=0.80, color='orange', linestyle='--', label='80% variance')
plt.legend()

# Find components needed for 95% variance
components_95 = np.argmax(explained_variance >= 0.95) + 1
plt.axvline(x=components_95, color='red', linestyle=':', alpha=0.5)
plt.text(components_95 + 5, 0.5, f'{components_95} components\nfor 95% variance',
         fontsize=10, color='red')

plt.savefig('../results/pca_variance.png', dpi=300)
plt.show()

# %%
# Save processed data for later use
print("\nSaving processed data...")
np.save('../data/simulated_leukemia_X.npy', X_leuk)
np.save('../data/simulated_leukemia_y.npy', y_leuk)
print("Data saved successfully!")

# %%
# Summary report
print("\n" + "="*60)
print("EXPLORATORY DATA ANALYSIS SUMMARY")
print("="*60)
print(f"\n1. Dataset Overview:")
print(f"   - Total datasets analyzed: {len(datasets_info)}")
print(f"   - Average features per sample: {summary_df['p_n_ratio'].mean():.1f}")
print(f"   - Most high-dimensional: {summary_df.loc[summary_df['p_n_ratio'].idxmax(), 'Dataset']}")
print(f"   - Least high-dimensional: {summary_df.loc[summary_df['p_n_ratio'].idxmin(), 'Dataset']}")

print(f"\n2. Leukemia Dataset Analysis:")
print(f"   - Shape: {X_leuk.shape}")
print(f"   - Dimensionality ratio: {X_leuk.shape[1] / X_leuk.shape[0]:.1f}")
print(f"   - Classes: {len(np.unique(y_leuk))}")
print(f"   - PCA: {pca.explained_variance_ratio_[0]:.1%} variance in PC1")
print(f"   - PCA: {pca.explained_variance_ratio_[1]:.1%} variance in PC2")

print(f"\n3. Key Insights from Paper:")
print(f"   - Challenge: 'Large p, small n' problem")
print(f"   - All datasets have features >> samples")
print(f"   - Need for feature selection is critical")
print(f"   - JMI performs best due to capturing interactions")

print("\n4. Recommendations for Analysis:")
print(f"   - Use feature selection before classification")
print(f"   - Consider JMI for capturing gene interactions")
print(f"   - Use 5-fold cross-validation for robustness")
print(f"   - Focus on biological interpretability of selected genes")